@misc{gao2024retrievalaugmented,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhang2023sirens,
  title         = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author        = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  year          = {2023},
  eprint        = {2309.01219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@online{metaairag,
  title  = {Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models},
  author = {Sebastian Riedel and Douwe Kiela and Patrick Lewis and Aleksandra Piktus},
  year   = {2020},
  url    = {\url{https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/}}
}

@online{datacampsupervised,
  title  = {Supervised Machine Learning},
  author = {Moez Ali},
  year   = {2022},
  url    = {\url{https://www.datacamp.com/blog/supervised-machine-learning}}
}

@online{awsrl,
  title  = {What is Reinforcement Learning?},
  author = {AWS},
  url    = {\url{https://aws.amazon.com/what-is/reinforcement-learning/#:~:text=Reinforcement%20learning%20(RL)%20is%20a,use%20to%20achieve%20their%20goals.}}
}

@online{ibmunsupervised,
  title  = {Supervised Machine Learning},
  author = {IBM},
  url    = {\url{https://www.ibm.com/topics/unsupervised-learning}}
}

@misc{datacamp:ml,
  author       = {{Matt Crabtree}},
  title        = {What is Machine Learning? Definition, Types, Tools \& More},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/blog/what-is-machine-learning}}
}

@misc{datacamp:dl,
  author       = {{Abid Ali Awan}},
  title        = {What is Deep Learning? A Tutorial for Beginners},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/tutorial/tutorial-deep-learning-tutorial}}
}

@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{openai:gpt,
  author       = {{Alec Radford}},
  title        = {Improving language understanding with unsupervised learning},
  year         = {2018},
  howpublished = {\url{https://openai.com/index/language-unsupervised}}
}

@article{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019},
  url    = {\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

@misc{raffel2023exploring,
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year          = {2023},
  eprint        = {1910.10683},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{brown2020language,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openai:tokens,
  author       = {{Openai help center}},
  title        = {What are tokens and how to count them?},
  year         = {2023},
  howpublished = {\url{https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them}}
}

@article{McCulloch1990ALC,
  title   = {A logical calculus of the ideas immanent in nervous activity},
  author  = {Warren S. McCulloch and Walter Pitts},
  journal = {Bulletin of Mathematical Biology},
  year    = {1990},
  volume  = {52},
  pages   = {99-115},
  url     = {\url{https://api.semanticscholar.org/CorpusID:15619658}}
}

@misc{oshea2015introduction,
  title         = {An Introduction to Convolutional Neural Networks},
  author        = {Keiron O'Shea and Ryan Nash},
  year          = {2015},
  eprint        = {1511.08458},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@misc{torfi2021natural,
  title         = {Natural Language Processing Advancements By Deep Learning: A Survey},
  author        = {Amirsina Torfi and Rouzbeh A. Shirvani and Yaser Keneshloo and Nader Tavaf and Edward A. Fox},
  year          = {2021},
  eprint        = {2003.01200},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{lipton2015critical,
  title         = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  author        = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  year          = {2015},
  eprint        = {1506.00019},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{nnanddl,
  title  = {Neural Networks and Deep Learning},
  author = {Michael Nielsen},
  year   = {2015},
  url    = {\url{https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/08/neuralnetworksanddeeplearning.pdf}}
}

@article{doi:10.1080/00207179008934126,
  author  = {S. CHEN, S. A. BILLINGS and P. M. GRANT},
  title   = {Non-linear system identification using neural networks},
  journal = {International Journal of Control},
  url     = {https://doi.org/10.1080/00207179008934126}
}

@misc{schmidt2019recurrent,
  title         = {Recurrent Neural Networks (RNNs): A gentle Introduction and Overview},
  author        = {Robin M. Schmidt},
  year          = {2019},
  eprint        = {1912.05911},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{58337,
  author   = {Werbos, P.J.},
  journal  = {Proceedings of the IEEE},
  title    = {Backpropagation through time: what it does and how to do it},
  year     = {1990},
  volume   = {78},
  number   = {10},
  pages    = {1550-1560},
  keywords = {Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi      = {10.1109/5.58337}
}

@article{10.1162/neco.1997.9.8.1735,
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title    = {{Long Short-Term Memory}},
  journal  = {Neural Computation},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  year     = {1997},
  month    = {11},
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1997.9.8.1735},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint   = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}

@misc{hoffmann2022training,
  title         = {Training Compute-Optimal Large Language Models},
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  year          = {2022},
  eprint        = {2203.15556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{6773263,
  author   = {Shannon, C. E.},
  journal  = {The Bell System Technical Journal},
  title    = {Prediction and entropy of printed English},
  year     = {1951},
  volume   = {30},
  number   = {1},
  pages    = {50-64},
  keywords = {},
  doi      = {10.1002/j.1538-7305.1951.tb01366.x}
}

@inproceedings{Jelinek1997StatisticalMF,
  title  = {Statistical methods for speech recognition},
  author = {Frederick Jelinek},
  year   = {1997},
  url    = {\url{https://api.semanticscholar.org/CorpusID:12495425}}
}


@book{Manning_Raghavan_Schütze_2008,
  place     = {Cambridge},
  title     = {Introduction to Information Retrieval},
  publisher = {Cambridge University Press},
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  year      = {2008}
}

@inproceedings{NIPS2000_728f206c,
  author    = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {T. Leen and T. Dietterich and V. Tresp},
  pages     = {},
  publisher = {MIT Press},
  title     = {A Neural Probabilistic Language Model},
  url       = {\url{https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf}},
  volume    = {13},
  year      = {2000}
}

@inproceedings{schwenk-etal-2006-continuous,
  title     = {Continuous Space Language Models for Statistical Machine Translation},
  author    = {Schwenk, Holger  and
               Dechelotte, Daniel  and
               Gauvain, Jean-Luc},
  booktitle = {Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions},
  month     = jul,
  year      = {2006},
  address   = {Sydney, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {\url{https://aclanthology.org/P06-2093}},
  pages     = {723--730}
}

@inproceedings{mikolov10_interspeech,
  author    = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
  title     = {{Recurrent neural network based language model}},
  year      = 2010,
  booktitle = {Proc. Interspeech 2010},
  pages     = {1045--1048},
  doi       = {10.21437/Interspeech.2010-343},
  issn      = {2308-457X}
}

@misc{graves2014generating,
  title         = {Generating Sequences With Recurrent Neural Networks},
  author        = {Alex Graves},
  year          = {2014},
  eprint        = {1308.0850},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@inproceedings{10.1145/2505515.2505665,
  author    = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
  title     = {Learning deep structured semantic models for web search using clickthrough data},
  year      = {2013},
  isbn      = {9781450322638},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {\url{https://doi.org/10.1145/2505515.2505665}},
  doi       = {10.1145/2505515.2505665},
  abstract  = {Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
  booktitle = {Proceedings of the 22nd ACM International Conference on Information \& Knowledge Management},
  pages     = {2333-2338},
  numpages  = {6},
  keywords  = {web search, semantic model, deep learning, clickthrough data},
  location  = {San Francisco, California, USA},
  series    = {CIKM '13}
}

@misc{gao2022neural,
  title         = {Neural Approaches to Conversational Information Retrieval},
  author        = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},
  year          = {2022},
  eprint        = {2201.05176},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{sutskever2014sequence,
  title         = {Sequence to Sequence Learning with Neural Networks},
  author        = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  year          = {2014},
  eprint        = {1409.3215},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{cho2014properties,
  title         = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  author        = {Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
  year          = {2014},
  eprint        = {1409.1259},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{vinyals2015tell,
  title         = {Show and Tell: A Neural Image Caption Generator},
  author        = {Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
  year          = {2015},
  eprint        = {1411.4555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{peters2018deep,
  title         = {Deep contextualized word representations},
  author        = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  year          = {2018},
  eprint        = {1802.05365},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{liu2019roberta,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year          = {2019},
  eprint        = {1907.11692},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{he2021deberta,
  title         = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author        = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  year          = {2021},
  eprint        = {2006.03654},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhou2023comprehensive,
  title         = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  author        = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
  year          = {2023},
  eprint        = {2302.09419},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{han2021pretrained,
  title         = {Pre-Trained Models: Past, Present and Future},
  author        = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  year          = {2021},
  eprint        = {2106.07139},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@article{Qiu_2020,
  title     = {Pre-trained models for natural language processing: A survey},
  volume    = {63},
  issn      = {1869-1900},
  url       = {http://dx.doi.org/10.1007/s11431-020-1647-3},
  doi       = {10.1007/s11431-020-1647-3},
  number    = {10},
  journal   = {Science China Technological Sciences},
  publisher = {Springer Science and Business Media LLC},
  author    = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  year      = {2020},
  month     = sep,
  pages     = {1872-1897}
}

@misc{chowdhery2022palm,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  year          = {2022},
  eprint        = {2204.02311},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{touvron2023llama,
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openai2024gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{wei2023chainofthought,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{mialon2023augmented,
  title         = {Augmented Language Models: a Survey},
  author        = {Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  year          = {2023},
  eprint        = {2302.07842},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{minaee2024large,
  title         = {Large Language Models: A Survey},
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year          = {2024},
  eprint        = {2402.06196},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{su2023roformer,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  year          = {2023},
  eprint        = {2104.09864},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{press2022train,
  title         = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author        = {Ofir Press and Noah A. Smith and Mike Lewis},
  year          = {2022},
  eprint        = {2108.12409},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{workshop2023bloom,
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author        = {BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  year          = {2023},
  eprint        = {2211.05100},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{ke2021rethinking,
  title         = {Rethinking Positional Encoding in Language Pre-training},
  author        = {Guolin Ke and Di He and Tie-Yan Liu},
  year          = {2021},
  eprint        = {2006.15595},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{shazeer2017outrageously,
  title         = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author        = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  year          = {2017},
  eprint        = {1701.06538},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{fedus2022switch,
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author        = {William Fedus and Barret Zoph and Noam Shazeer},
  year          = {2022},
  eprint        = {2101.03961},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{mahabadi2021parameterefficient,
  title         = {Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks},
  author        = {Rabeeh Karimi Mahabadi and Sebastian Ruder and Mostafa Dehghani and James Henderson},
  year          = {2021},
  eprint        = {2106.04489},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhang2024instruction,
  title         = {Instruction Tuning for Large Language Models: A Survey},
  author        = {Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  year          = {2024},
  eprint        = {2308.10792},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{mishra2022crosstask,
  title         = {Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
  author        = {Swaroop Mishra and Daniel Khashabi and Chitta Baral and Hannaneh Hajishirzi},
  year          = {2022},
  eprint        = {2104.08773},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{ouyang2022training,
  title         = {Training language models to follow instructions with human feedback},
  author        = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  year          = {2022},
  eprint        = {2203.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{alpaca,
  author       = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title        = {Stanford Alpaca: An Instruction-following LLaMA model},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}}
}

@misc{wang2023selfinstruct,
  title         = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author        = {Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  year          = {2023},
  eprint        = {2212.10560},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{christiano2023deep,
  title         = {Deep reinforcement learning from human preferences},
  author        = {Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
  year          = {2023},
  eprint        = {1706.03741},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@misc{schulman2017proximal,
  title         = {Proximal Policy Optimization Algorithms},
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  year          = {2017},
  eprint        = {1707.06347},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{ahmadian2024basics,
  title         = {Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs},
  author        = {Arash Ahmadian and Chris Cremer and Matthias Gallé and Marzieh Fadaee and Julia Kreutzer and Olivier Pietquin and Ahmet Üstün and Sara Hooker},
  year          = {2024},
  eprint        = {2402.14740},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{Ji_2023,
  title     = {Survey of Hallucination in Natural Language Generation},
  volume    = {55},
  issn      = {1557-7341},
  url       = {http://dx.doi.org/10.1145/3571730},
  doi       = {10.1145/3571730},
  number    = {12},
  journal   = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  year      = {2023},
  month     = mar,
  pages     = {1-38}
}

@misc{mckenna2023sources,
  title         = {Sources of Hallucination by Large Language Models on Inference Tasks},
  author        = {Nick McKenna and Tianyi Li and Liang Cheng and Mohammad Javad Hosseini and Mark Johnson and Mark Steedman},
  year          = {2023},
  eprint        = {2305.14552},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{yao2023tree,
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author        = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  year          = {2023},
  eprint        = {2305.10601},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{manakul2023selfcheckgpt,
  title         = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author        = {Potsawee Manakul and Adian Liusie and Mark J. F. Gales},
  year          = {2023},
  eprint        = {2303.08896},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{shinn2023reflexion,
  title         = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  author        = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  year          = {2023},
  eprint        = {2303.11366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{wu2022promptchainer,
  title         = {PromptChainer: Chaining Large Language Model Prompts through Visual Programming},
  author        = {Tongshuang Wu and Ellen Jiang and Aaron Donsbach and Jeff Gray and Alejandra Molina and Michael Terry and Carrie J Cai},
  year          = {2022},
  eprint        = {2203.06566},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}

@misc{zhou2023large,
  title         = {Large Language Models Are Human-Level Prompt Engineers},
  author        = {Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
  year          = {2023},
  eprint        = {2211.01910},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{lewis2021retrievalaugmented,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
  year          = {2021},
  eprint        = {2005.11401},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{jiang2023active,
  title         = {Active Retrieval Augmented Generation},
  author        = {Zhengbao Jiang and Frank F. Xu and Luyu Gao and Zhiqing Sun and Qian Liu and Jane Dwivedi-Yu and Yiming Yang and Jamie Callan and Graham Neubig},
  year          = {2023},
  eprint        = {2305.06983},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{schick2023toolformer,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  year          = {2023},
  eprint        = {2302.04761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{patil2023gorilla,
  title         = {Gorilla: Large Language Model Connected with Massive APIs},
  author        = {Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
  year          = {2023},
  eprint        = {2305.15334},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{kandpal2023large,
  title         = {Large Language Models Struggle to Learn Long-Tail Knowledge},
  author        = {Nikhil Kandpal and Haikang Deng and Adam Roberts and Eric Wallace and Colin Raffel},
  year          = {2023},
  eprint        = {2211.08411},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{arora2023garmeetsrag,
  title         = {GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval},
  author        = {Daman Arora and Anush Kini and Sayak Ray Chowdhury and Nagarajan Natarajan and Gaurav Sinha and Amit Sharma},
  year          = {2023},
  eprint        = {2310.20158},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{borgeaud2022improving,
  title         = {Improving language models by retrieving from trillions of tokens},
  author        = {Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},
  year          = {2022},
  eprint        = {2112.04426},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{ma2023query,
  title         = {Query Rewriting for Retrieval-Augmented Large Language Models},
  author        = {Xinbei Ma and Yeyun Gong and Pengcheng He and Hai Zhao and Nan Duan},
  year          = {2023},
  eprint        = {2305.14283},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{peng2024large,
  title         = {Large Language Model based Long-tail Query Rewriting in Taobao Search},
  author        = {Wenjun Peng and Guiyang Li and Yue Jiang and Zilong Wang and Dan Ou and Xiaoyi Zeng and Derong Xu and Tong Xu and Enhong Chen},
  year          = {2024},
  eprint        = {2311.03758},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{gao2022precise,
  title         = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
  author        = {Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
  year          = {2022},
  eprint        = {2212.10496},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{yu2023generate,
  title         = {Generate rather than Retrieve: Large Language Models are Strong Context Generators},
  author        = {Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
  year          = {2023},
  eprint        = {2209.10063},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{shao2023enhancing,
  title         = {Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy},
  author        = {Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
  year          = {2023},
  eprint        = {2305.15294},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@online{googlevectorembeddings,
  title  = {Meet AI's multitool: Vector embeddings},
  author = {Dale Markowitz},
  year   = {2022},
  url    = {\url{https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings}}
}

@online{practicalrag,
  title  = {A Practical Approach to Retrieval Augmented Generation Systems},
  author = {Mehdi Allahyari, Angelina Yang},
  year   = {2023},
  url    = {\url{https://mallahyari.github.io/rag-ebook/}}
}