@misc{gao2024retrievalaugmented,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhang2023sirens,
  title         = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author        = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  year          = {2023},
  eprint        = {2309.01219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@online{metaairag,
  title  = {Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models},
  author = {Sebastian Riedel and Douwe Kiela and Patrick Lewis and Aleksandra Piktus},
  year   = {2020},
  url    = {\url{https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/}}
}

@online{datacampsupervised,
  title  = {Supervised Machine Learning},
  author = {Moez Ali},
  year   = {2022},
  url    = {\url{https://www.datacamp.com/blog/supervised-machine-learning}}
}

@online{awsrl,
  title  = {What is Reinforcement Learning?},
  author = {AWS},
  url    = {\url{https://aws.amazon.com/what-is/reinforcement-learning/#:~:text=Reinforcement%20learning%20(RL)%20is%20a,use%20to%20achieve%20their%20goals.}}
}

@online{ibmunsupervised,
  title  = {Supervised Machine Learning},
  author = {IBM},
  url    = {\url{https://www.ibm.com/topics/unsupervised-learning}}
}

@misc{datacamp:ml,
  author       = {{Matt Crabtree}},
  title        = {What is Machine Learning? Definition, Types, Tools \& More},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/blog/what-is-machine-learning}}
}

@misc{datacamp:dl,
  author       = {{Abid Ali Awan}},
  title        = {What is Deep Learning? A Tutorial for Beginners},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/tutorial/tutorial-deep-learning-tutorial}}
}

@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{openai:gpt,
  author       = {{Alec Radford}},
  title        = {Improving language understanding with unsupervised learning},
  year         = {2018},
  howpublished = {\url{https://openai.com/index/language-unsupervised}}
}

@article{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019},
  url    = {\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

@misc{raffel2023exploring,
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year          = {2023},
  eprint        = {1910.10683},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{brown2020language,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openai:tokens,
  author       = {{Openai help center}},
  title        = {What are tokens and how to count them?},
  year         = {2023},
  howpublished = {\url{https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them}}
}

@article{McCulloch1990ALC,
  title   = {A logical calculus of the ideas immanent in nervous activity},
  author  = {Warren S. McCulloch and Walter Pitts},
  journal = {Bulletin of Mathematical Biology},
  year    = {1990},
  volume  = {52},
  pages   = {99-115},
  url     = {\url{https://api.semanticscholar.org/CorpusID:15619658}}
}

@misc{oshea2015introduction,
  title         = {An Introduction to Convolutional Neural Networks},
  author        = {Keiron O'Shea and Ryan Nash},
  year          = {2015},
  eprint        = {1511.08458},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@misc{torfi2021natural,
  title         = {Natural Language Processing Advancements By Deep Learning: A Survey},
  author        = {Amirsina Torfi and Rouzbeh A. Shirvani and Yaser Keneshloo and Nader Tavaf and Edward A. Fox},
  year          = {2021},
  eprint        = {2003.01200},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{lipton2015critical,
  title         = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  author        = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  year          = {2015},
  eprint        = {1506.00019},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{nnanddl,
  title  = {Neural Networks and Deep Learning},
  author = {Michael Nielsen},
  year   = {2015},
  url    = {\url{https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/08/neuralnetworksanddeeplearning.pdf}}
}

@article{doi:10.1080/00207179008934126,
  author  = {S. CHEN, S. A. BILLINGS and P. M. GRANT},
  title   = {Non-linear system identification using neural networks},
  journal = {International Journal of Control},
  url     = {https://doi.org/10.1080/00207179008934126}
}

@misc{schmidt2019recurrent,
  title         = {Recurrent Neural Networks (RNNs): A gentle Introduction and Overview},
  author        = {Robin M. Schmidt},
  year          = {2019},
  eprint        = {1912.05911},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{58337,
  author   = {Werbos, P.J.},
  journal  = {Proceedings of the IEEE},
  title    = {Backpropagation through time: what it does and how to do it},
  year     = {1990},
  volume   = {78},
  number   = {10},
  pages    = {1550-1560},
  keywords = {Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi      = {10.1109/5.58337}
}

@article{10.1162/neco.1997.9.8.1735,
  author   = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
  title    = {{Long Short-Term Memory}},
  journal  = {Neural Computation},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  year     = {1997},
  month    = {11},
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1997.9.8.1735},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint   = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}
