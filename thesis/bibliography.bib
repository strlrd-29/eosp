@misc{gao2024retrievalaugmented,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhang2023sirens,
  title         = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author        = {Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  year          = {2023},
  eprint        = {2309.01219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@online{metaairag,
  title  = {Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models},
  author = {Sebastian Riedel and Douwe Kiela and Patrick Lewis and Aleksandra Piktus},
  year   = {2020},
  url    = {\url{https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/}}
}

@online{datacampsupervised,
  title  = {Supervised Machine Learning},
  author = {Moez Ali},
  year   = {2022},
  url    = {\url{https://www.datacamp.com/blog/supervised-machine-learning}}
}

@online{awsrl,
  title  = {What is Reinforcement Learning?},
  author = {AWS},
  url    = {\url{https://aws.amazon.com/what-is/reinforcement-learning/#:~:text=Reinforcement%20learning%20(RL)%20is%20a,use%20to%20achieve%20their%20goals.}}
}

@online{ibmunsupervised,
  title  = {Supervised Machine Learning},
  author = {IBM},
  url    = {\url{https://www.ibm.com/topics/unsupervised-learning}}
}

@misc{datacamp:ml,
  author       = {{Matt Crabtree}},
  title        = {What is Machine Learning? Definition, Types, Tools \& More},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/blog/what-is-machine-learning}}
}

@misc{datacamp:dl,
  author       = {{Abid Ali Awan}},
  title        = {What is Deep Learning? A Tutorial for Beginners},
  year         = {2023},
  howpublished = {\url{https://www.datacamp.com/tutorial/tutorial-deep-learning-tutorial}}
}

@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{openai:gpt,
  author       = {{Alec Radford}},
  title        = {Improving language understanding with unsupervised learning},
  year         = {2018},
  howpublished = {\url{https://openai.com/index/language-unsupervised}}
}

@article{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019},
  url    = {\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

@misc{raffel2023exploring,
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year          = {2023},
  eprint        = {1910.10683},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{brown2020language,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openai:tokens,
  author       = {{Openai help center}},
  title        = {What are tokens and how to count them?},
  year         = {2023},
  howpublished = {\url{https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them}}
}

@article{McCulloch1990ALC,
  title   = {A logical calculus of the ideas immanent in nervous activity},
  author  = {Warren S. McCulloch and Walter Pitts},
  journal = {Bulletin of Mathematical Biology},
  year    = {1990},
  volume  = {52},
  pages   = {99-115},
  url     = {\url{https://api.semanticscholar.org/CorpusID:15619658}}
}

@misc{oshea2015introduction,
  title         = {An Introduction to Convolutional Neural Networks},
  author        = {Keiron O'Shea and Ryan Nash},
  year          = {2015},
  eprint        = {1511.08458},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@misc{torfi2021natural,
  title         = {Natural Language Processing Advancements By Deep Learning: A Survey},
  author        = {Amirsina Torfi and Rouzbeh A. Shirvani and Yaser Keneshloo and Nader Tavaf and Edward A. Fox},
  year          = {2021},
  eprint        = {2003.01200},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{lipton2015critical,
  title         = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  author        = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  year          = {2015},
  eprint        = {1506.00019},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{nnanddl,
  title  = {Neural Networks and Deep Learning},
  author = {Michael Nielsen},
  year   = {2015},
  url    = {\url{https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2022/08/neuralnetworksanddeeplearning.pdf}}
}

@article{doi:10.1080/00207179008934126,
  author  = {S. CHEN, S. A. BILLINGS and P. M. GRANT},
  title   = {Non-linear system identification using neural networks},
  journal = {International Journal of Control},
  url     = {https://doi.org/10.1080/00207179008934126}
}

@misc{schmidt2019recurrent,
  title         = {Recurrent Neural Networks (RNNs): A gentle Introduction and Overview},
  author        = {Robin M. Schmidt},
  year          = {2019},
  eprint        = {1912.05911},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{58337,
  author   = {Werbos, P.J.},
  journal  = {Proceedings of the IEEE},
  title    = {Backpropagation through time: what it does and how to do it},
  year     = {1990},
  volume   = {78},
  number   = {10},
  pages    = {1550-1560},
  keywords = {Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi      = {10.1109/5.58337}
}

@article{10.1162/neco.1997.9.8.1735,
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title    = {{Long Short-Term Memory}},
  journal  = {Neural Computation},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  year     = {1997},
  month    = {11},
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1997.9.8.1735},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint   = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}

@misc{hoffmann2022training,
  title         = {Training Compute-Optimal Large Language Models},
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  year          = {2022},
  eprint        = {2203.15556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{6773263,
  author   = {Shannon, C. E.},
  journal  = {The Bell System Technical Journal},
  title    = {Prediction and entropy of printed English},
  year     = {1951},
  volume   = {30},
  number   = {1},
  pages    = {50-64},
  keywords = {},
  doi      = {10.1002/j.1538-7305.1951.tb01366.x}
}

@inproceedings{Jelinek1997StatisticalMF,
  title  = {Statistical methods for speech recognition},
  author = {Frederick Jelinek},
  year   = {1997},
  url    = {\url{https://api.semanticscholar.org/CorpusID:12495425}}
}


@book{Manning_Raghavan_Schütze_2008,
  place     = {Cambridge},
  title     = {Introduction to Information Retrieval},
  publisher = {Cambridge University Press},
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  year      = {2008}
}

@inproceedings{NIPS2000_728f206c,
  author    = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {T. Leen and T. Dietterich and V. Tresp},
  pages     = {},
  publisher = {MIT Press},
  title     = {A Neural Probabilistic Language Model},
  url       = {\url{https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf}},
  volume    = {13},
  year      = {2000}
}

@inproceedings{schwenk-etal-2006-continuous,
  title     = {Continuous Space Language Models for Statistical Machine Translation},
  author    = {Schwenk, Holger  and
               Dechelotte, Daniel  and
               Gauvain, Jean-Luc},
  booktitle = {Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions},
  month     = jul,
  year      = {2006},
  address   = {Sydney, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {\url{https://aclanthology.org/P06-2093}},
  pages     = {723--730}
}

@inproceedings{mikolov10_interspeech,
  author    = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
  title     = {{Recurrent neural network based language model}},
  year      = 2010,
  booktitle = {Proc. Interspeech 2010},
  pages     = {1045--1048},
  doi       = {10.21437/Interspeech.2010-343},
  issn      = {2308-457X}
}

@misc{graves2014generating,
  title         = {Generating Sequences With Recurrent Neural Networks},
  author        = {Alex Graves},
  year          = {2014},
  eprint        = {1308.0850},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}

@inproceedings{10.1145/2505515.2505665,
  author    = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
  title     = {Learning deep structured semantic models for web search using clickthrough data},
  year      = {2013},
  isbn      = {9781450322638},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {\url{https://doi.org/10.1145/2505515.2505665}},
  doi       = {10.1145/2505515.2505665},
  abstract  = {Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
  booktitle = {Proceedings of the 22nd ACM International Conference on Information \& Knowledge Management},
  pages     = {2333-2338},
  numpages  = {6},
  keywords  = {web search, semantic model, deep learning, clickthrough data},
  location  = {San Francisco, California, USA},
  series    = {CIKM '13}
}

@misc{gao2022neural,
  title         = {Neural Approaches to Conversational Information Retrieval},
  author        = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},
  year          = {2022},
  eprint        = {2201.05176},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{sutskever2014sequence,
  title         = {Sequence to Sequence Learning with Neural Networks},
  author        = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  year          = {2014},
  eprint        = {1409.3215},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{cho2014properties,
  title         = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  author        = {Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
  year          = {2014},
  eprint        = {1409.1259},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{vinyals2015tell,
  title         = {Show and Tell: A Neural Image Caption Generator},
  author        = {Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
  year          = {2015},
  eprint        = {1411.4555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{peters2018deep,
  title         = {Deep contextualized word representations},
  author        = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  year          = {2018},
  eprint        = {1802.05365},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{liu2019roberta,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year          = {2019},
  eprint        = {1907.11692},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{he2021deberta,
  title         = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author        = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  year          = {2021},
  eprint        = {2006.03654},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhou2023comprehensive,
  title         = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  author        = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
  year          = {2023},
  eprint        = {2302.09419},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{han2021pretrained,
  title         = {Pre-Trained Models: Past, Present and Future},
  author        = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  year          = {2021},
  eprint        = {2106.07139},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@article{Qiu_2020,
  title     = {Pre-trained models for natural language processing: A survey},
  volume    = {63},
  issn      = {1869-1900},
  url       = {http://dx.doi.org/10.1007/s11431-020-1647-3},
  doi       = {10.1007/s11431-020-1647-3},
  number    = {10},
  journal   = {Science China Technological Sciences},
  publisher = {Springer Science and Business Media LLC},
  author    = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  year      = {2020},
  month     = sep,
  pages     = {1872-1897}
}

@misc{chowdhery2022palm,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  year          = {2022},
  eprint        = {2204.02311},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{touvron2023llama,
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openai2024gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{wei2023chainofthought,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{mialon2023augmented,
  title         = {Augmented Language Models: a Survey},
  author        = {Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  year          = {2023},
  eprint        = {2302.07842},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{minaee2024large,
  title         = {Large Language Models: A Survey},
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year          = {2024},
  eprint        = {2402.06196},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}