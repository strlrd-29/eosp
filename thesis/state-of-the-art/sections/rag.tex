\section{Retrieval Augmented Generation (RAG)}

\subsection{Introduction}

Large language models (LLMs) have demonstrated significant success, yet they continue to encounter substantial limitations, particularly in tasks that are domain-specific or knowledge-intensive \cite{kandpal2023large}. A prominent issue is the generation of "hallucinations" \cite{zhang2023sirens}, where LLMs produce responses to queries that exceed their training data or necessitate up-to-date information. To mitigate these challenges, Retrieval-Augmented Generation (RAG) improves LLM performance by retrieving pertinent document chunks from an external knowledge base through semantic similarity calculations. By incorporating external knowledge, RAG substantially reduces the likelihood of generating factually incorrect content. The integration of RAG into LLMs has become widespread, establishing it as a crucial technology for advancing chatbots and enhancing the practical applicability of LLMs in real-world scenarios.

To address the limitations of generative AI, researchers and engineers have developed innovative methods, including the Retrieval-Augmented Generation (RAG) approach. RAG gained significant attention among generative AI developers following the release of the seminal paper "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" by Lewis et al. (2020) at Facebook AI Research  \cite{lewis2021retrievalaugmented}. RAG enhances the quality and relevance of generated text by combining the strengths of generative AI with retrieval techniques. Unlike traditional generative models that depend solely on their internal knowledge, RAG incorporates an additional step of retrieving information from external sources, such as databases, documents, or the web, before generating a response. This integration enables RAG to access up-to-date information and context, making it especially valuable for applications requiring accurate and current information.

RAG technology has seen rapid development in recent years, as illustrated by the technology tree summarizing related research in Figure \ref{fig:rag-tree}. The evolution of RAG within the era of large models can be divided into several distinct stages. Initially, the inception of RAG coincided with the rise of the Transformer architecture, focusing on improving language models by integrating additional knowledge through Pre-Training Models (PTM). This early phase was marked by foundational efforts to refine pre-training techniques \cite{arora2023garmeetsrag}-\cite{borgeaud2022improving}. The subsequent emergence of ChatGPT \cite{ouyang2022training} represented a pivotal moment, showcasing powerful in-context learning (ICL) capabilities. Following this, RAG research shifted towards enhancing LLMs' ability to address more complex and knowledge-intensive tasks during the inference stage, spurring rapid advancements in RAG studies. Over time, the focus expanded beyond the inference stage to include fine-tuning techniques for LLMs, further enhancing RAG's capabilities.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{rag-tree.png}
    \caption{
        \it{ Technology tree of RAG research \cite{gao2024retrievalaugmented}}
    }
    \label{fig:rag-tree}
\end{figure}

\subsection{Retrieval Augmented Generation (RAG) Categories}

The RAG research paradigm is continuously evolving and can be categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG, as shown in Figure \ref{fig:rag-categories}. Although RAG methods are cost-effective and outperform native LLMs, they also present several limitations. The development of Advanced RAG and Modular RAG addresses these specific shortcomings found in Naive RAG.

\subsubsection*{Naive RAG}

The Naive RAG research paradigm represents the earliest methodology, gaining prominence soon after the widespread adoption of ChatGPT. Naive RAG follows a traditional process encompassing indexing, retrieval, and generation, often described as a "Retrieve-Read" framework \cite{ma2023query}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]{rag-categories.png}
    \caption{
        \it{ Comparison between the three paradigms of RAG \cite{gao2024retrievalaugmented}.}
    }
    \label{fig:rag-categories}
\end{figure}

Indexing starts with cleaning and extracting raw data from diverse formats like PDF, HTML, Word, and Markdown, converting it into a uniform plain text format. To address the context limitations of language models, the text is broken down into smaller, manageable chunks. These chunks are then encoded into vector representations using an embedding model and stored in a vector database. This step is critical for enabling efficient similarity searches in the next retrieval phase.

Upon receiving a user query, the RAG system employs the same encoding model used during the indexing phase to convert the query into a vector representation. It then calculates similarity scores between the query vector and the vectors of the chunks in the indexed corpus. The system prioritizes and retrieves the top K chunks that show the highest similarity to the query. These chunks are then used as the expanded context in the prompt for the generation phase.

During the generation phase, the posed query and the selected documents are combined into a coherent prompt. A large language model then formulates a response based on this prompt. The model's approach to answering can vary depending on task-specific criteria, allowing it to either utilize its inherent parametric knowledge or restrict its responses to the information within the provided documents. In cases of ongoing dialogues, any existing conversational history can be incorporated into the prompt, enabling the model to effectively engage in multi-turn interactions.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]{naive-rag.png}
    \caption{
        \it{Naive RAG}
    }
    \label{fig:naive-rag}
\end{figure}

\subsubsection*{Advanced RAG}

Advanced RAG introduces specific enhancements to address the limitations of Naive RAG. It focuses on improving retrieval quality through both pre-retrieval and post-retrieval strategies. To resolve indexing issues, Advanced RAG employs refined indexing techniques such as a sliding window approach, fine-grained segmentation, and the integration of metadata. Additionally, it incorporates various optimization methods to streamline the retrieval process.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]{advanced-rag.png}
    \caption{
        \it{Advanced RAG}
    }
    \label{fig:advanced-rag}
\end{figure}

Pre-retrieval process. In this stage, the primary focus is on optimizing both the indexing structure and the original query. The objective of indexing optimization is to enhance the quality of the content being indexed. This involves several strategies: increasing data granularity, optimizing index structures, incorporating metadata, alignment optimization, and mixed retrieval. The goal of query optimization is to make the user's original question clearer and more suitable for retrieval. Common methods include query rewriting, query transformation, query expansion, and other techniques \cite{ma2023query}, \cite{peng2024large} \cite{gao2022precise}.

Post-Retrieval Process. Once relevant context is retrieved, it is crucial to integrate it effectively with the query. The main methods in the post-retrieval process include reranking chunks and compressing the context. Re-ranking involves adjusting the order of the retrieved information to ensure the most relevant content is positioned at the forefront of the prompt. This strategy is implemented in frameworks such as LlamaIndex\footnote{\url{https://www.llamaindex.ai/}}, LangChain\footnote{\url{https://www.langchain.com/}}, and HayStack\footnote{\url{https://haystack.deepset.ai/}}. Directly feeding all relevant documents into LLMs can cause information overload, diluting the focus on key details with irrelevant content. To mitigate this, post-retrieval efforts concentrate on selecting essential information, highlighting critical sections, and reducing the context to a manageable size for processing.

\subsubsection*{Modular RAG}

The modular RAG architecture advances beyond the earlier RAG paradigms, providing enhanced adaptability and versatility. It incorporates various strategies to improve its components, such as adding a search module for similarity searches and fine-tuning the retriever. Innovations like restructured RAG modules \cite{yu2023generate} and rearranged RAG pipelines \cite{shao2023enhancing} have been developed to address specific challenges. The trend towards a modular RAG approach is becoming increasingly common, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds on the foundational principles of Advanced and Naive RAG, demonstrating a progression and refinement within the RAG family.

\subsection{Retrieval Models for Document Search}

Selecting the appropriate retrieval model is critical for ensuring efficient and accurate document search and ranking in response to user queries. Here are some key retrieval models and considerations:

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency):}
TF-IDF is a classical retrieval model that calculates the importance of terms within a document relative to a corpus. It is straightforward to implement and particularly effective for certain tasks, especially within smaller or less complex datasets. This model is suitable for straightforward keyword-based search tasks, offering a simple yet effective approach for basic retrieval needs.

\textbf{BM25:}
BM25 builds upon TF-IDF by accounting for document length and term saturation, making it an improved version of its predecessor. It handles term frequency variations and document length normalization better, often resulting in more effective retrieval for modern tasks. BM25 is ideal for general-purpose search engines and more sophisticated retrieval needs, providing a robust solution for a variety of applications.

\textbf{Vector Space Models:}
These models represent documents and queries as vectors in a high-dimensional space, using metrics like cosine similarity for ranking. Implementations such as Latent Semantic Analysis (LSA) and Word Embeddings (e.g., Word2Vec) can capture semantic similarities and relationships between terms. This makes vector space models effective for tasks that require understanding nuanced meanings and context within documents.

\textbf{Neural Ranking Models:}
Modern neural models, such as those based on BERT, capture complex semantic relationships and provide deep understanding of context and nuances. These models can be fine-tuned for specific tasks and domains, making them highly effective for advanced retrieval needs. Neural ranking models are particularly suitable for tasks requiring sophisticated semantic comprehension and detailed query resolution.

\textbf{Hybrid Models:}
Hybrid models combine multiple retrieval approaches, such as TF-IDF with neural models, leveraging the strengths of each method. By balancing simplicity and effectiveness, hybrid models offer robustness across various scenarios, making them useful for comprehensive retrieval systems that need to perform well in a range of conditions.

\subsection{Embeddings and Vector Databases for Retrieval in RAG}

In addition to selecting an appropriate retrieval model, leveraging embeddings and vector databases can significantly enhance the performance and efficiency of the retrieval component. Vector embeddings are a fundamental concept in modern information retrieval and natural language processing. They transform textual data into numerical vectors, enabling computers to understand and manipulate text data within a mathematical, geometric space. These embeddings capture semantic and contextual relationships between words, documents, and other textual entities, making them highly valuable in various applications, including the retrieval component of Retrieval-Augmented Generation (RAG). By employing embeddings and vector databases, the retrieval process becomes more efficient and effective, providing more accurate and contextually relevant results.

\subsubsection*{Vector Embeddings}

Vector embeddings represent words, phrases, sentences, or even entire documents as points in a high-dimensional vector space. The key idea is to map each textual element into a vector in such a way that semantically similar elements are located close to each other in this space, while dissimilar elements are further apart. This geometric representation facilitates similarity calculations, clustering, and other operations \cite{practicalrag}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]{embeddings.png}
    \caption[Word Embeddings]{
        \it{Words plotted in 3-dimensional space. Embeddings can have hundreds or thousands of dimensions-too many for humans to visualize \cite{googlevectorembeddings}.}
    }
\end{figure}

Examples of Vector Embeddings:

\begin{itemize}
    \item \textbf{Word Embeddings (Word2Vec, GloVe):} Word embeddings represent individual words as vectors. For example, “king” and “queen” may be represented as vectors that are close together in the vector space because they share similar semantic properties.
    \item \textbf{Document Embeddings (Doc2Vec, BERT):} Document embeddings map entire documents (such as PDFs) into vectors. Two documents discussing similar topics will have embeddings that are close in the vector space.
\end{itemize}

\subsubsection*{Vector Databases and Their Role in Enhancing Retrieval}

Vector databases, also known as similarity search engines or vector index databases, play a crucial role in the retrieval component of RAG by efficiently storing and retrieving these vector embeddings. They are specialized databases designed for retrieving vectors based on similarity, making them well-suited for scenarios where similarity between data points needs to be calculated quickly and accurately.

How Vector Databases Enhance Retrieval in RAG:

\textbf{Fast Retrieval:} Vector databases utilize indexing structures specifically optimized for similarity searches. They employ algorithms such as approximate nearest neighbor (ANN) search to quickly identify the most similar vectors, even within large datasets containing numerous documents.

\textbf{Scalability:} Vector databases are designed to efficiently scale as the document corpus expands. This ensures that retrieval performance remains consistent, regardless of the dataset's size, maintaining high efficiency and speed.

\textbf{Advanced Similarity Scoring:} These databases provide a variety of similarity metrics, including cosine similarity and the Jaccard index. This allows for fine-tuning the relevance ranking of retrieved documents based on specific requirements, enhancing the precision of search results.

\textbf{Integration with Retrieval Models:} Vector databases can be seamlessly integrated into existing retrieval systems. They complement retrieval models like TF-IDF, BM25, and neural ranking models by offering an efficient method for selecting candidate documents based on vector similarity, thereby improving the overall retrieval process.

These factors have led to the emergence of numerous new vector databases. Choosing one of these databases can create long-term dependencies and have significant impacts on your system. Ideally, a vector database should exhibit strong scalability while maintaining cost-efficiency and minimizing latency. Some of these vector databases include Qdrant\footnote{\url{https://qdrant.tech/}}, Weaviate\footnote{\url{https://weaviate.io/}}, Pinecone\footnote{\url{https://www.pinecone.io/}}, pgvector\footnote{\url{https://github.com/pgvector/pgvector}}, Milvus\footnote{\url{https://milvus.io/}}, and Chroma\footnote{\url{https://www.trychroma.com/}}.

\subsection{Challenges of Retrieval-Augmented Generation}

The adoption of Retrieval-Augmented Generation (RAG) marks a significant advancement in natural language processing and information retrieval. However, like any complex AI system, RAG presents a set of challenges that must be addressed to fully harness its potential. This section explores some of the key challenges associated with RAG.

\subsubsection*{Data Quality and Relevance}

RAG heavily relies on the availability of high-quality and relevant data for both retrieval and generation tasks. Challenges in this area include:

\begin{itemize}
    \item \textbf{Noisy Data:} Incomplete, outdated, or inaccurate data sources can lead to the retrieval of irrelevant information, negatively impacting the quality of generated responses.

    \item \textbf{Bias and Fairness:} Biases present in training data may result in biased retrieval and generation, perpetuating stereotypes or misinformation.
\end{itemize}

\subsubsection*{Integration Complexity}

Integrating retrieval and generation components seamlessly is a complex task, as it involves bridging different architectures and models. Challenges include:

\begin{itemize}
    \item \textbf{Model Compatibility}: Ensuring that the retrieval and generation models work harmoniously, particularly when combining traditional methods (e.g., TF-IDF) with neural models (e.g., GPT-3).

    \item \textbf{Latency and Efficiency:} Balancing the need for real-time responsiveness with the computational resources required for both retrieval and generation.
\end{itemize}


\subsubsection*{Scalability}

Scaling RAG systems to handle large volumes of data and user requests presents several challenges:

\begin{itemize}
    \item \textbf{Indexing Efficiency:} As the document corpus grows, maintaining an efficient and up-to-date index becomes crucial for ensuring retrieval speed.

    \item \textbf{Model Scaling:} Deploying large-scale neural models for both retrieval and generation demands substantial computational resources.
\end{itemize}

\subsubsection*{Domain Adaptation}

Adapting RAG systems to specific domains or industries can be complex:

\begin{itemize}
    \item \textbf{Domain-Specific Knowledge:} Incorporating industry-specific knowledge and terminology into retrieval and generation.

    \item \textbf{Training Data Availability:} Ensuring the availability of domain-specific training data for fine-tuning models.
\end{itemize}